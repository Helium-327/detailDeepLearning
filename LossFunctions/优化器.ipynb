{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 常见的训练优化器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. SGD (Stochastic Gradient Descent)\n",
    "\n",
    "**原理：**\n",
    "\n",
    "SGD 是最简单的优化器之一，它使用随机梯度下降的方法来更新模型的参数。SGD 的更新规则是：\n",
    "\n",
    "`w = w - lr * ∇L(w)`\n",
    "\n",
    "其中 `w` 是模型的参数，`lr` 是学习率，`∇L(w)` 是损失函数的梯度。\n",
    "\n",
    "**优缺点：**\n",
    "\n",
    "- 优点：SGD 简单易实现，计算效率高。\n",
    "- 缺点：SGD 的收敛速度慢，可能会陷入局部最优。\n",
    "\n",
    "**参数使用：**\n",
    "\n",
    "* 学习率 (`lr`): 推荐值为 0.01-0.1\n",
    "\n",
    "**使用案例：**\n",
    "\n",
    "> SGD 适用于简单的模型和小规模的数据集。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Momentum SGD\n",
    "\n",
    "**原理：**\n",
    "\n",
    "> Momentum SGD 是 SGD 的变种，它在更新规则中添加了一个动量项来帮助模型摆脱局部最优。Momentum SGD 的更新规则是：\n",
    "\n",
    "`w = w - lr * ∇L(w) + momentum * v`\n",
    "\n",
    "其中 `v` 是动量项，`momentum` 是动量的系数。\n",
    "\n",
    "**优缺点：**\n",
    "\n",
    "- 优点：Momentum SGD 可以帮助模型摆脱局部最优，收敛速度比 SGD 快。\n",
    "- 缺点：Momentum SGD 的参数需要谨慎选择。\n",
    "\n",
    "**参数使用：**\n",
    "\n",
    "* 学习率 (`lr`): 推荐值为 0.01-0.1\n",
    "* 动量 (`momentum`): 推荐值为 0.9-0.99\n",
    "\n",
    "**使用案例：**\n",
    "\n",
    "> Momentum SGD 适用于需要摆脱局部最优的模型和数据集。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Nesterov Accelerated Gradient (NAG)\n",
    "\n",
    "**原理：**\n",
    "\n",
    "> NAG 是 Momentum SGD 的变种，它在更新规则中添加了一个校正项来帮助模型更快地收敛。NAG 的更新规则是：\n",
    "\n",
    "`w = w - lr * ∇L(w) + momentum * v - lr * ∇L(w + momentum * v)`\n",
    "\n",
    "**优缺点：**\n",
    "\n",
    "- 优点：NAG 可以帮助模型更快地收敛，收敛速度比 Momentum SGD 快。\n",
    "- 缺点：NAG 的参数需要谨慎选择。\n",
    "\n",
    "**参数使用：**\n",
    "\n",
    "* 学习率 (`lr`): 推荐值为 0.01-0.1\n",
    "* 动量 (`momentum`): 推荐值为 0.9-0.99\n",
    "\n",
    "**使用案例：**\n",
    "\n",
    "> NAG 适用于需要快速收敛的模型和数据集。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Adam\n",
    "\n",
    "**原理：**\n",
    "\n",
    "Adam 是一个自适应的优化器，它可以自动调整学习率和动量。Adam 的更新规则是：\n",
    "\n",
    "`w = w - lr * ∇L(w) / sqrt(v + epsilon)`\n",
    "\n",
    "其中 `v` 是动量项，`epsilon` 是一个小的正值。\n",
    "\n",
    "**优缺点：**\n",
    "\n",
    "- 优点：Adam 可以自动调整学习率和动量，收敛速度快。\n",
    "- 缺点：Adam 的参数需要谨慎选择。\n",
    "\n",
    "**参数使用：**\n",
    "\n",
    "* 学习率 (`lr`): 推荐值为 0.001-0.01\n",
    "* 动量 (`beta1`): 推荐值为 0.9-0.99\n",
    "* 动量 (`beta2`): 推荐值为 0.99-0.999\n",
    "\n",
    "**使用案例：**\n",
    "\n",
    "> Adam 适用于需要自动调整学习率和动量的模型和数据集。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. RMSProp\n",
    "\n",
    "**原理：**\n",
    "\n",
    "> RMSProp 是一个自适应的优化器，它可以自动调整学习率。RMSProp 的更新规则是：\n",
    "\n",
    "`w = w - lr * ∇L(w) / sqrt(v + epsilon)`\n",
    "\n",
    "其中 `v` 是动量项，`epsilon` 是一个小的正值。\n",
    "\n",
    "**优缺点：**\n",
    "\n",
    "- 优点：RMSProp 可以自动调整学习率，收敛速度快。\n",
    "- 缺点：RMSProp 的参数需要谨慎选择。\n",
    "\n",
    "**参数使用：**\n",
    "\n",
    "* 学习率 (`lr`): 推荐值为 0.001-0.01\n",
    "* 动量 (`gamma`): 推荐值为 0.9-0.99\n",
    "\n",
    "**使用案例：**\n",
    "\n",
    "> RMSProp 适用于需要自动调整学习率的模型和数据集。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Adagrad\n",
    "\n",
    "**原理：**\n",
    "\n",
    "Adagrad 是一个自适应的优化器，它可以自动调整学习率。Adagrad 的更新规则是：\n",
    "\n",
    "`w = w - lr * ∇L(w) / sqrt(v + epsilon)`\n",
    "\n",
    "其中 `v` 是动量项，`epsilon` 是一个小的正值。\n",
    "\n",
    "**优缺点：**\n",
    "\n",
    "- 优点：Adagrad 可以自动调整学习率，收敛速度快。\n",
    "\n",
    "- 缺点：Adagrad 的参数需要谨慎选择。\n",
    "\n",
    "**参数使用：**\n",
    "\n",
    "* 学习率 (`lr`): 推荐值为 0.001-0.01\n",
    "* 动量 (`gamma`): 推荐值为 0.9-0.99\n",
    "\n",
    "**使用案例：**\n",
    "\n",
    "> Adagrad 适用于需要自动调整学习率的模型和数据集。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Adadelta\n",
    "\n",
    "**原理：**\n",
    "\n",
    "Adadelta 是一个自适应的优化器，它可以自动调整学习率。Adadelta 的更新规则是：\n",
    "\n",
    "`w = w - lr * ∇L(w) / sqrt(v + epsilon)`\n",
    "\n",
    "其中 `v` 是动量项，`epsilon` 是一个小的正值。\n",
    "\n",
    "**优缺点：**\n",
    "\n",
    "- 优点：Adadelta 可以自动调整学习率，收敛速度快。\n",
    "- 缺点：Adadelta 的参数需要谨慎选择。\n",
    "\n",
    "**参数使用：**\n",
    "\n",
    "* 学习率 (`lr`): 推荐值为 0.001-0.01\n",
    "* 动量 (`gamma`): 推荐值为 0.9-0.99\n",
    "\n",
    "**使用案例：**\n",
    "\n",
    "> Adadelta 适用于需要自动调整学习率的模型和数据集。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Nadam\n",
    "\n",
    "**原理：**\n",
    "\n",
    "Nadam 是 Adam 的变种，它可以自动调整学习率和动量。Nadam 的更新规则是：\n",
    "\n",
    "`w = w - lr * ∇L(w) / sqrt(v + epsilon)`\n",
    "\n",
    "其中 `v` 是动量项，`epsilon` 是一个小的正值。\n",
    "\n",
    "**优缺点：**\n",
    "\n",
    "- 优点：Nadam 可以自动调整学习率和动量，收敛速度快。\n",
    "- 缺点：Nadam 的参数需要谨慎选择。\n",
    "\n",
    "**参数使用：**\n",
    "\n",
    "* 学习率 (`lr`): 推荐值为 0.001-0.01\n",
    "* 动量 (`beta1`): 推荐值为 0.9-0.99\n",
    "* 动量 (`beta2`): 推荐值为 0.99-0.999\n",
    "\n",
    "**使用案例：**\n",
    "\n",
    "> Nadam 适用于需要自动调整学习率和动量的模型和数据集。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Adamax\n",
    "\n",
    "**原理：**\n",
    "\n",
    "Adamax 是 Adam 的变种，它可以自动调整学习率和动量。Adamax 的更新规则是：\n",
    "\n",
    "`w = w - lr * ∇L(w) / sqrt(v + epsilon)`\n",
    "\n",
    "其中 `v` 是动量项，`epsilon` 是一个小的正值。\n",
    "\n",
    "**优缺点：**\n",
    "\n",
    "- 优点：Adamax 可以自动调整学习率和动量，收敛速度快。\n",
    "- 缺点：Adamax 的参数需要谨慎选择。\n",
    "\n",
    "**参数使用：**\n",
    "\n",
    "* 学习率 (`lr`): 推荐值为 0.001-0.01\n",
    "* 动量 (`beta1`): 推荐值为 0.9-0.99\n",
    "* 动量 (`beta2`): 推荐值为 0.99-0.999\n",
    "\n",
    "**使用案例：**\n",
    "\n",
    "> Adamax 适用于需要自动调整学习率和动量的模型和数据集。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. ASGD\n",
    "\n",
    "**原理：**\n",
    "\n",
    "ASGD 是一个自适应的优化器，它可以自动调整学习率。ASGD 的更新规则是：\n",
    "\n",
    "`w = w - lr * ∇L(w) / sqrt(v + epsilon)`\n",
    "\n",
    "其中 `v` 是动量项，`epsilon` 是一个小的正值。\n",
    "\n",
    "**优缺点：**\n",
    "\n",
    "- 优点：ASGD 可以自动调整学习率，收敛速度快。\n",
    "- 缺点：ASGD 的参数需要谨慎选择。\n",
    "\n",
    "**参数使用：**\n",
    "\n",
    "* 学习率 (`lr`): 推荐值为 0.001-0.01\n",
    "* 动量 (`gamma`): 推荐值为 0.9-0.99\n",
    "\n",
    "**使用案例：**\n",
    "\n",
    "> ASGD 适用于需要自动调整学习率的模型和数据集。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
